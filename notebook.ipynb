{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "274979b6-5a29-4a7e-8a0c-e07491aa9de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Final Project**\n",
    "\n",
    "This is a project doing some basic data analysis of IMDB movie data and associated wiki streaming events. It should be completed by groups of no less than 2 students and no more than 4 students. Each member of the group should have at least a few commits associated in the project repo.\n",
    "\n",
    "## **Scoring**\n",
    "\n",
    "The code must run and provide the correct answers . 1/2 points\n",
    "The remainder will come from notebook organization, code comments, etc .\n",
    "For the questions that have answers, please also provide those in markdown cells in the notebook, and/or part of a mardown file in the repo .\n",
    "All relevant code should be shared via a shared Git repository. Additionally, you will send an email to joe@adaltas.com when the project has been submitted . Please ensure that the names of all participants are included in the repo and in the submission email . Note: For full credit the code must run with little to no extra input from the end user, and, any extra input that is required must be clearly documented and explained. Also note, any question that is at least attempted will be awarded with partial credit provided there is a corresponding explanation of the difficulties faced.\n",
    "\n",
    "## **Questions**\n",
    "\n",
    "  1 - load data from here. This should be done using a notebook cell and not a manual process to import the data. NOTE: You may not need all of the datasets, but you will be utilizing most of them.\n",
    "\n",
    "  2 - How many total people in data set?\n",
    "\n",
    "  3 - What is the earliest year of birth?\n",
    "\n",
    "  4 - How many years ago was this person born?\n",
    "\n",
    "  5 - Using only the data in the data set, determine if this date of birth correct.\n",
    "\n",
    "  6 - Explain the reasoning for the answer in a code comment or new markdown cell.\n",
    "\n",
    "  7 - What is the most recent data of birth?\n",
    "\n",
    "  8 - What percentage of the people do not have a listed date of birth?\n",
    "\n",
    "  9 - What is the length of the longest \"short\" after 1900?\n",
    "\n",
    "  10 - What is the length of the shortest \"movie\" after 1900?\n",
    "\n",
    "  11 - List of all of the genres represented.\n",
    "\n",
    "  12 - What is the higest rated comedy \"movie\" in the dataset? Note, if there is a tie, the tie shall be broken by the movie with the most votes.\n",
    "\n",
    "  13 - Who was the director of the movie?\n",
    "\n",
    "  14 - List, if any, the alternate titles for the movie.\n",
    "\n",
    "## **Stream Processing**\n",
    "\n",
    "Choose any five entities from the data set. These can be specific movies, actors, crews, etc, or more abstract concepts such as specific genres, etc. The main criteria is that the entities chosen must have a trackable wiki page. Set up a stream processing job that will track events for the chosen entities from the wikimedia Events Platform. These tracking jobs should provide some simple metrics. These metrics should be stored in a database or file (depending on the platform used). At least one of the metrics should be of the \"alert\" type (meaning some event that would require further action. For instance imagine wanting to be notified each time a specific user makes a change. Capture this \"alert\" and mimic an alerting system by routing these events to a different file/database.) These tables/data do not need to be shared, but the structure of the output should be clearly noted in the code and/or markdown cells. Additionally, a brief explanation/overview of this section should be provided in a seperate markdown cell or in the project readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be89187-c2eb-4224-a5ea-c83d9c370d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ffc95d-433d-4c91-b994-d6d0d18df9b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Population Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f308db-ea66-4f9c-a9f3-0dc8aacfed8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create folder if not exists\n",
    "import os\n",
    "\n",
    "local_path = \"/Workspace/Users/mael.depreville@edu.ece.fr/big-data-processing-project/data\"\n",
    "\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "local_path\n",
    "\n",
    "# 2. Download files using shell \n",
    "files = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    \"title.episode.tsv.gz\",\n",
    "    # \"title.principals.tsv.gz\",                        # Too big for instance to run it in Databricks\n",
    "    \"title.ratings.tsv.gz\"\n",
    "]\n",
    "\n",
    "dict_files_names = {\n",
    "    \"name.basics.tsv.gz\": \"name.basics\",\n",
    "    \"title.akas.tsv.gz\": \"title.akas\",\n",
    "    \"title.basics.tsv.gz\": \"title.basics\",\n",
    "    \"title.crew.tsv.gz\": \"title.crew\",\n",
    "    \"title.episode.tsv.gz\": \"title.episode\",\n",
    "    # \"title.principals.tsv.gz\": \"title.principals\",    # Too big for instance to run it in Databricks\n",
    "    \"title.ratings.tsv.gz\": \"title.ratings\"\n",
    "}\n",
    "\n",
    "base_url = \"https://datasets.imdbws.com/\"\n",
    "\n",
    "for f in files:\n",
    "    url = base_url + f\n",
    "    out = f\"{local_path}/{f}\"\n",
    "    print(\"Downloading:\", f)\n",
    "    os.system(f\"wget -O {out} {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d166d5ed-1014-4b7e-95ed-877e42702933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict_df = {}\n",
    "\n",
    "for file_name in files:\n",
    "    df = (spark.read\n",
    "          .option(\"compression\", \"gzip\")\n",
    "          .option(\"inferSchema\", \"false\")\n",
    "          .option(\"nullValue\", \"\\\\N\")\n",
    "          .csv(f\"{local_path}/{file_name}\", header=True, sep=\"\\t\"))\n",
    "    df.show(5)\n",
    "    dict_df[dict_files_names[file_name]] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cabf0ae-09b8-4634-b92a-c17f0201d627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9635bb32-371e-4762-ada9-a96605a1ecdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73454473-de8c-4936-83a2-7c94f05e4008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514485f2-c4ba-4d96-bfef-be79ef9bb384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_name_count = (\n",
    "  dict_df[\"name.basics\"]\n",
    "  .select(\"primaryName\")\n",
    "  .distinct()\n",
    "  .count()\n",
    ")\n",
    "display(unique_name_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd5b1338-513c-497f-b4af-46206ef7a765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e8972f9-8542-4a44-91b1-c0b9013133cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There is a major problem here since the original dataset provides us with dates in absolute values as the example below proves it with the date of birth of Cesar (-100 -> 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f253c17-018a-420d-a0a0-e34750984863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw = spark.read.text(f\"{local_path}/name.basics.tsv.gz\")\n",
    "raw.filter(raw.value.contains(\"Gaio Giulio Cesare\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d395fd7-6f26-4e8e-b6c8-6a5f7273ea5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-> We have then the date of birth closest to 0 in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a561a6-c265-41d5-b995-2c78c9a2734b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "min_birth_year = (\n",
    "  dict_df[\"name.basics\"]\n",
    "  .select(min(col(\"birthYear\").cast(\"double\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "min_birth_year_df = dict_df[\"name.basics\"].filter(col(\"birthYear\") == min_birth_year)\n",
    "\n",
    "min_birth_year_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70138e1d-bca6-4c26-9f56-7093efcea4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4f2911-1871-4d05-ac9b-0da0bdfd996f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "current_year = date.today().year\n",
    "years_difference = int(current_year - min_birth_year)\n",
    "print(f\"The difference between the current year and the earliest date of birth in our dataset is {years_difference} years!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52693bd-7ab7-4f42-b7e9-ab9e40327d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f4666d-1df0-411d-825c-d0e3d0fc0064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "058ba95c-a250-4af0-94a7-369998a9a0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e951405f-b76f-4d15-a886-bfd7556966b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If we consider this question as a question about the veracity of the earliest date of birth in this dataset, we have answered it a bit above (indicating that the date values were absolute -> preventing us from finding the earliest one but allowing us to find the closest to 0)\n",
    "\n",
    "- On another hand, if we consider this question as a question about how we can check the veracity of the date of birth of this person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5082feb5-c380-4442-98fb-f9a25f6371dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b544a972-3dc4-4ff8-b980-5c356de3b2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "max_birth_year = (\n",
    "  dict_df[\"name.basics\"]\n",
    "  .select(max(col(\"birthYear\").cast(\"double\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "max_birth_year_df = dict_df[\"name.basics\"].filter(col(\"birthYear\") == max_birth_year)\n",
    "\n",
    "max_birth_year_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8cebf1d-2235-4dd7-a631-07783113d3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**8.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a6a546-2970-41a3-b26d-d2e39af6a15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_rows = dict_df[\"name.basics\"].count()\n",
    "null_rows = dict_df[\"name.basics\"].filter(col(\"birthYear\").isNull()).count()\n",
    "\n",
    "birth_year_null_pct = (null_rows / total_rows) * 100\n",
    "\n",
    "print(f\"{birth_year_null_pct:.2f}% of the people in this dataset do not have a listed date of birth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e26f28-3560-4bcd-9f2f-265908e1c6bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**9.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d17e53-83ee-46fa-9d4e-388837da99b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "longest_short_after_1900 = (\n",
    "  dict_df[\"title.basics\"]\n",
    "  .filter((col(\"titleType\") == \"short\") & (col(\"startYear\") >= 1900))\n",
    "  .select(max(col(\"runtimeMinutes\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "print(f\"The longest short film after 1900 was {longest_short_after_1900} minutes long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efef10d5-bbac-4614-ad8c-0fb2977777f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**10.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052cc621-907c-4e6d-a44c-4f706d4e32a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "shortest_movie_after_1900 = (\n",
    "  dict_df[\"title.basics\"]\n",
    "  .filter((col(\"titleType\") == \"movie\") & (col(\"startYear\") >= 1900))\n",
    "  .select(min(col(\"runtimeMinutes\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "print(f\"The shortest movie film after 1900 was {shortest_movie_after_1900} minutes long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e98aa31-84ea-4ea2-a797-147788fdb22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**11.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8900dec7-f36f-474e-840d-33c4e8d6816b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, trim, col\n",
    "\n",
    "genres_df = (\n",
    "    dict_df[\"title.basics\"]\n",
    "    .select(explode(split(col(\"genres\"), \",\")).alias(\"genre\"))\n",
    "    .select(trim(col(\"genre\")).alias(\"genre\"))\n",
    "    .filter(col(\"genre\").isNotNull() & (col(\"genre\") != \"\"))\n",
    ")\n",
    "\n",
    "unique_genres = [row[\"genre\"] for row in genres_df.select(\"genre\").distinct().collect()]\n",
    "\n",
    "print(unique_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1cc79f-5ad7-433b-bce7-c19a68c4af69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**12.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803561e0-052e-4ebd-a618-0962e08ed360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_joined = dict_df[\"title.basics\"].join(\n",
    "    dict_df[\"title.ratings\"],\n",
    "    on=\"tconst\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "df_filtered = df_joined.filter((col(\"titleType\") == \"movie\") & (col(\"genres\").contains(\"Comedy\")))\n",
    "w = Window.orderBy(desc(\"averageRating\"), desc(\"numVotes\"))\n",
    "\n",
    "highest_rated_comedy_movie = (\n",
    "    df_filtered\n",
    "    .withColumn(\"rank\", dense_rank().over(w))\n",
    "    .filter(col(\"rank\") == 1)\n",
    "    .drop(\"rank\")\n",
    ")\n",
    "\n",
    "highest_rated_comedy_movie.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1757e235-ef10-4dc3-b550-9cd7714b7e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**13.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b76e1a7-3820-40f0-b73e-69cdebf75076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "highest_rated_comedy_movie_director_df = (\n",
    "    highest_rated_comedy_movie\n",
    "    .join(dict_df[\"title.crew\"], on=\"tconst\", how=\"inner\")\n",
    "    .join(dict_df[\"name.basics\"], dict_df[\"title.crew\"].directors == dict_df[\"name.basics\"].nconst, how=\"inner\")\n",
    "    .select(\"primaryName\").alias(\"Director\")\n",
    ")\n",
    "\n",
    "display(highest_rated_comedy_movie_director_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32bd84c7-91f9-47a1-816a-516ce7cebd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**14**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c4ca3a5-64cb-489d-99c0-f95992acfb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For this last question we suspect that it was based on the fields provided in the _title.principals_ dataset (that we couldn't download at the beginning of this notebook since it causes OOM in Databricks...)\n",
    "\n",
    "But we could easily imagine that the query to obtain the alternate titles would be something like:  \n",
    "<br>\n",
    "```\n",
    "highest_rated_comedy_movie_titles_df = (\n",
    "    highest_rated_comedy_movie\n",
    "    .join(dict_df[\"title.principals\"], on=\"tconst\", how=\"inner\")\n",
    "    .select(\"primaryTitle\", \"alternatesTitle\")\n",
    ")\n",
    "\n",
    "display(highest_rated_comedy_movie_titles_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09194b8-bbdb-4b2b-ac2c-6d550db37cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05e7ff7-bc5a-4ed3-9237-e24390c1c220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Stream Processing**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
