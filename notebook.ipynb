{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "274979b6-5a29-4a7e-8a0c-e07491aa9de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Final Project**\n",
    "\n",
    "This is a project doing some basic data analysis of IMDB movie data and associated wiki streaming events. It should be completed by groups of no less than 2 students and no more than 4 students. Each member of the group should have at least a few commits associated in the project repo.\n",
    "\n",
    "## **Scoring**\n",
    "\n",
    "The code must run and provide the correct answers . 1/2 points\n",
    "The remainder will come from notebook organization, code comments, etc .\n",
    "For the questions that have answers, please also provide those in markdown cells in the notebook, and/or part of a mardown file in the repo .\n",
    "All relevant code should be shared via a shared Git repository. Additionally, you will send an email to joe@adaltas.com when the project has been submitted . Please ensure that the names of all participants are included in the repo and in the submission email . Note: For full credit the code must run with little to no extra input from the end user, and, any extra input that is required must be clearly documented and explained. Also note, any question that is at least attempted will be awarded with partial credit provided there is a corresponding explanation of the difficulties faced.\n",
    "\n",
    "## **Questions**\n",
    "\n",
    "  1 - load data from here. This should be done using a notebook cell and not a manual process to import the data. NOTE: You may not need all of the datasets, but you will be utilizing most of them.\n",
    "\n",
    "  2 - How many total people in data set?\n",
    "\n",
    "  3 - What is the earliest year of birth?\n",
    "\n",
    "  4 - How many years ago was this person born?\n",
    "\n",
    "  5 - Using only the data in the data set, determine if this date of birth correct.\n",
    "\n",
    "  6 - Explain the reasoning for the answer in a code comment or new markdown cell.\n",
    "\n",
    "  7 - What is the most recent data of birth?\n",
    "\n",
    "  8 - What percentage of the people do not have a listed date of birth?\n",
    "\n",
    "  9 - What is the length of the longest \"short\" after 1900?\n",
    "\n",
    "  10 - What is the length of the shortest \"movie\" after 1900?\n",
    "\n",
    "  11 - List of all of the genres represented.\n",
    "\n",
    "  12 - What is the higest rated comedy \"movie\" in the dataset? Note, if there is a tie, the tie shall be broken by the movie with the most votes.\n",
    "\n",
    "  13 - Who was the director of the movie?\n",
    "\n",
    "  14 - List, if any, the alternate titles for the movie.\n",
    "\n",
    "## **Stream Processing**\n",
    "\n",
    "Choose any five entities from the data set. These can be specific movies, actors, crews, etc, or more abstract concepts such as specific genres, etc. The main criteria is that the entities chosen must have a trackable wiki page. Set up a stream processing job that will track events for the chosen entities from the wikimedia Events Platform. These tracking jobs should provide some simple metrics. These metrics should be stored in a database or file (depending on the platform used). At least one of the metrics should be of the \"alert\" type (meaning some event that would require further action. For instance imagine wanting to be notified each time a specific user makes a change. Capture this \"alert\" and mimic an alerting system by routing these events to a different file/database.) These tables/data do not need to be shared, but the structure of the output should be clearly noted in the code and/or markdown cells. Additionally, a brief explanation/overview of this section should be provided in a seperate markdown cell or in the project readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be89187-c2eb-4224-a5ea-c83d9c370d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ffc95d-433d-4c91-b994-d6d0d18df9b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Population Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f308db-ea66-4f9c-a9f3-0dc8aacfed8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: name.basics.tsv.gz\n",
      "Downloading: title.akas.tsv.gz\n",
      "Downloading: title.basics.tsv.gz\n",
      "Downloading: title.crew.tsv.gz\n",
      "Downloading: title.episode.tsv.gz\n",
      "Downloading: title.ratings.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "sh: wget: command not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "local_path = \"./data\"                                                                                             \n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "local_path\n",
    "\n",
    "# 2. Download files using shell \n",
    "files = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    \"title.episode.tsv.gz\",\n",
    "    # \"title.principals.tsv.gz\",                        # Too big for instance to run it in Databricks\n",
    "    \"title.ratings.tsv.gz\"\n",
    "]\n",
    "\n",
    "dict_files_names = {\n",
    "    \"name.basics.tsv.gz\": \"name.basics\",\n",
    "    \"title.akas.tsv.gz\": \"title.akas\",\n",
    "    \"title.basics.tsv.gz\": \"title.basics\",\n",
    "    \"title.crew.tsv.gz\": \"title.crew\",\n",
    "    \"title.episode.tsv.gz\": \"title.episode\",\n",
    "    # \"title.principals.tsv.gz\": \"title.principals\",    # Too big for instance to run it in Databricks\n",
    "    \"title.ratings.tsv.gz\": \"title.ratings\"\n",
    "}\n",
    "\n",
    "base_url = \"https://datasets.imdbws.com/\"\n",
    "\n",
    "for f in files:\n",
    "    url = base_url + f\n",
    "    out = f\"{local_path}/{f}\"\n",
    "    print(\"Downloading:\", f)\n",
    "    os.system(f\"wget -O {out} {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d166d5ed-1014-4b7e-95ed-877e42702933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME = /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|    primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|nm0000001|   Fred Astaire|     1899|     1987|actor,miscellaneo...|tt0072308,tt00504...|\n",
      "|nm0000002|  Lauren Bacall|     1924|     2014|actress,miscellan...|tt0037382,tt00752...|\n",
      "|nm0000003|Brigitte Bardot|     1934|     NULL|actress,music_dep...|tt0057345,tt00491...|\n",
      "|nm0000004|   John Belushi|     1949|     1982|actor,writer,musi...|tt0072562,tt00779...|\n",
      "|nm0000005| Ingmar Bergman|     1918|     2007|writer,director,a...|tt0050986,tt00694...|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|  titleId|ordering|               title|region|language|      types|   attributes|isOriginalTitle|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|tt0000001|       1|          Carmencita|  NULL|    NULL|   original|         NULL|              1|\n",
      "|tt0000001|       2|          Carmencita|    DE|    NULL|       NULL|literal title|              0|\n",
      "|tt0000001|       3|          Carmencita|    US|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       4|Carmencita - span...|    HU|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       5|          Καρμενσίτα|    GR|    NULL|imdbDisplay|         NULL|              0|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.17\" 2025-10-21\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.17+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.17+0, mixed mode, sharing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|   NULL|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|   NULL|             5|     Animation,Short|\n",
      "|tt0000003|    short|        Poor Pierrot|      Pauvre Pierrot|      0|     1892|   NULL|             5|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|   NULL|            12|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|   NULL|             1|               Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "+---------+---------+---------+\n",
      "|   tconst|directors|  writers|\n",
      "+---------+---------+---------+\n",
      "|tt0000001|nm0005690|     NULL|\n",
      "|tt0000002|nm0721526|     NULL|\n",
      "|tt0000003|nm0721526|nm0721526|\n",
      "|tt0000004|nm0721526|     NULL|\n",
      "|tt0000005|nm0005690|     NULL|\n",
      "+---------+---------+---------+\n",
      "only showing top 5 rows\n",
      "+---------+------------+------------+-------------+\n",
      "|   tconst|parentTconst|seasonNumber|episodeNumber|\n",
      "+---------+------------+------------+-------------+\n",
      "|tt0031458|  tt32857063|        NULL|         NULL|\n",
      "|tt0041951|   tt0041038|           1|            9|\n",
      "|tt0042816|   tt0989125|           1|           17|\n",
      "|tt0042889|   tt0989125|        NULL|         NULL|\n",
      "|tt0043426|   tt0040051|           3|           42|\n",
      "+---------+------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    2189|\n",
      "|tt0000002|          5.5|     309|\n",
      "|tt0000003|          6.4|    2276|\n",
      "|tt0000004|          5.1|     197|\n",
      "|tt0000005|          6.2|    3014|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "print(\"JAVA_HOME =\", os.environ.get(\"JAVA_HOME\"))\n",
    "os.system(\"java -version\")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"IMDB Analysis\").getOrCreate()\n",
    "dict_df = {}\n",
    "for file_name in files:\n",
    "    df = (spark.read\n",
    "          .option(\"compression\", \"gzip\")\n",
    "          .option(\"inferSchema\", \"false\")\n",
    "          .option(\"nullValue\", \"\\\\N\")\n",
    "          .csv(f\"{local_path}/{file_name}\", header=True, sep=\"\\t\"))\n",
    "    df.show(5)\n",
    "    dict_df[dict_files_names[file_name]] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cabf0ae-09b8-4634-b92a-c17f0201d627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9635bb32-371e-4762-ada9-a96605a1ecdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73454473-de8c-4936-83a2-7c94f05e4008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514485f2-c4ba-4d96-bfef-be79ef9bb384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11416176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_name_count = (\n",
    "  dict_df[\"name.basics\"]\n",
    "  .select(\"primaryName\")\n",
    "  .distinct()\n",
    "  .count()\n",
    ")\n",
    "display(unique_name_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd5b1338-513c-497f-b4af-46206ef7a765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e8972f9-8542-4a44-91b1-c0b9013133cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There is a major problem here since the original dataset provides us with dates in absolute values as the example below proves it with the date of birth of Cesar (-100 -> 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f253c17-018a-420d-a0a0-e34750984863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 228:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+\n",
      "|value                                                                              |\n",
      "+-----------------------------------------------------------------------------------+\n",
      "|nm2471712\\tGaio Giulio Cesare\\t100\\t44\\twriter,archive_footage\\ttt0191909,tt0057105|\n",
      "+-----------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw = spark.read.text(f\"{local_path}/name.basics.tsv.gz\")\n",
    "raw.filter(raw.value.contains(\"Gaio Giulio Cesare\")).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d395fd7-6f26-4e8e-b6c8-6a5f7273ea5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-> We have then the date of birth closest to 0 in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a561a6-c265-41d5-b995-2c78c9a2734b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 232:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------+---------+-----------------+--------------------+\n",
      "|   nconst|       primaryName|birthYear|deathYear|primaryProfession|      knownForTitles|\n",
      "+---------+------------------+---------+---------+-----------------+--------------------+\n",
      "|nm0784172|Lucio Anneo Seneca|        4|       65|           writer|tt0043802,tt02188...|\n",
      "+---------+------------------+---------+---------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "min_birth_year = (\n",
    "  dict_df[\"name.basics\"]\n",
    "  .select(min(col(\"birthYear\").cast(\"double\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "min_birth_year_df = dict_df[\"name.basics\"].filter(col(\"birthYear\") == min_birth_year)\n",
    "\n",
    "min_birth_year_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70138e1d-bca6-4c26-9f56-7093efcea4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4f2911-1871-4d05-ac9b-0da0bdfd996f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between the current year and the earliest date of birth in our dataset is 2021 years!\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "current_year = date.today().year\n",
    "years_difference = int(current_year - min_birth_year)\n",
    "print(f\"The difference between the current year and the earliest date of birth in our dataset is {years_difference} years!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52693bd-7ab7-4f42-b7e9-ab9e40327d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f4666d-1df0-411d-825c-d0e3d0fc0064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 234:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age at first work: 1947 years - Birth year seems SUSPICIOUS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "person_nconst = min_birth_year_df.select(\"nconst\").first()[0]\n",
    "\n",
    "person_works = (\n",
    "    dict_df[\"title.crew\"]\n",
    "    .filter(col(\"directors\").contains(person_nconst) | col(\"writers\").contains(person_nconst))\n",
    "    .join(dict_df[\"title.basics\"], on=\"tconst\")\n",
    "    .select(\"startYear\")\n",
    "    .filter(col(\"startYear\").isNotNull())\n",
    "    .orderBy(\"startYear\")\n",
    ")\n",
    "\n",
    "first_work = person_works.first()\n",
    "if first_work:\n",
    "    age = int(first_work[0]) - int(min_birth_year)\n",
    "    verdict = \"INCORRECT\" if age < 0 else \"SUSPICIOUS\" if age < 10 or age > 150 else \"plausible\"\n",
    "    print(f\"Age at first work: {age} years - Birth year seems {verdict}\")\n",
    "else:\n",
    "    print(\"No works found to verify birth year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "058ba95c-a250-4af0-94a7-369998a9a0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e951405f-b76f-4d15-a886-bfd7556966b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- If we consider this question as a question about the veracity of the earliest date of birth in this dataset, we have answered it a bit above (indicating that the date values were absolute -> preventing us from finding the earliest one but allowing us to find the closest to 0)\n",
    "\n",
    "- On another hand, if we consider this question as a question about how we can check the veracity of the date of birth of this person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5082feb5-c380-4442-98fb-f9a25f6371dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b544a972-3dc4-4ff8-b980-5c356de3b2ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 242:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------+---------+--------------------+--------------------+\n",
      "|    nconst|      primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+----------+-----------------+---------+---------+--------------------+--------------------+\n",
      "|nm16784939|Kyrah Ivy Jackson|     2025|     NULL|             actress|                NULL|\n",
      "| nm5642311|     Chase Ramsey|     2025|     NULL|actor,director,wr...|tt17505010,tt1471...|\n",
      "+----------+-----------------+---------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "max_birth_year = (\n",
    "  dict_df[\"name.basics\"]\n",
    "  .select(max(col(\"birthYear\").cast(\"double\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "max_birth_year_df = dict_df[\"name.basics\"].filter(col(\"birthYear\") == max_birth_year)\n",
    "\n",
    "max_birth_year_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8cebf1d-2235-4dd7-a631-07783113d3af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**8.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a6a546-2970-41a3-b26d-d2e39af6a15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 246:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.58% of the people in this dataset do not have a listed date of birth!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_rows = dict_df[\"name.basics\"].count()\n",
    "null_rows = dict_df[\"name.basics\"].filter(col(\"birthYear\").isNull()).count()\n",
    "\n",
    "birth_year_null_pct = (null_rows / total_rows) * 100\n",
    "\n",
    "print(f\"{birth_year_null_pct:.2f}% of the people in this dataset do not have a listed date of birth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1e26f28-3560-4bcd-9f2f-265908e1c6bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**9.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d17e53-83ee-46fa-9d4e-388837da99b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 249:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest short film after 1900 was 97 minutes long!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, max\n",
    "\n",
    "longest_short_after_1900 = (\n",
    "  dict_df[\"title.basics\"]\n",
    "  .filter((col(\"titleType\") == \"short\") & (col(\"startYear\") >= 1900))\n",
    "  .select(max(col(\"runtimeMinutes\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "print(f\"The longest short film after 1900 was {longest_short_after_1900} minutes long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efef10d5-bbac-4614-ad8c-0fb2977777f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**10.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052cc621-907c-4e6d-a44c-4f706d4e32a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 252:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest movie film after 1900 was 1 minutes long!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "shortest_movie_after_1900 = (\n",
    "  dict_df[\"title.basics\"]\n",
    "  .filter((col(\"titleType\") == \"movie\") & (col(\"startYear\") >= 1900))\n",
    "  .select(min(col(\"runtimeMinutes\")))\n",
    "  .collect()[0][0]\n",
    ")\n",
    "\n",
    "print(f\"The shortest movie film after 1900 was {shortest_movie_after_1900} minutes long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e98aa31-84ea-4ea2-a797-147788fdb22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**11.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8900dec7-f36f-474e-840d-33c4e8d6816b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 255:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crime', 'Romance', 'Thriller', 'Adventure', 'Drama', 'War', 'Documentary', 'Reality-TV', 'Family', 'Fantasy', 'Game-Show', 'Adult', 'History', 'Mystery', 'Musical', 'Animation', 'Music', 'Film-Noir', 'Short', 'Horror', 'Western', 'Biography', 'Comedy', 'Sport', 'Action', 'Talk-Show', 'Sci-Fi', 'News']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, trim, col\n",
    "\n",
    "genres_df = (\n",
    "    dict_df[\"title.basics\"]\n",
    "    .select(explode(split(col(\"genres\"), \",\")).alias(\"genre\"))\n",
    "    .select(trim(col(\"genre\")).alias(\"genre\"))\n",
    "    .filter(col(\"genre\").isNotNull() & (col(\"genre\") != \"\"))\n",
    ")\n",
    "\n",
    "unique_genres = [row[\"genre\"] for row in genres_df.select(\"genre\").distinct().collect()]\n",
    "\n",
    "print(unique_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1cc79f-5ad7-433b-bce7-c19a68c4af69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**12.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803561e0-052e-4ebd-a618-0962e08ed360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 20:18:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:18:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:18:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:18:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:18:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 259:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+--------------+-------+---------+-------+--------------+-------------------+-------------+--------+\n",
      "|tconst    |titleType|primaryTitle  |originalTitle |isAdult|startYear|endYear|runtimeMinutes|genres             |averageRating|numVotes|\n",
      "+----------+---------+--------------+--------------+-------+---------+-------+--------------+-------------------+-------------+--------+\n",
      "|tt38949436|movie    |Itlu Me Yedava|Itlu Me Yedava|0      |2025     |NULL   |132           |Comedy,Drama,Family|9.7          |2195    |\n",
      "+----------+---------+--------------+--------------+-------+---------+-------+--------------+-------------------+-------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 20:18:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:18:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_joined = dict_df[\"title.basics\"].join(\n",
    "    dict_df[\"title.ratings\"],\n",
    "    on=\"tconst\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Filtre: movies + Comedy + minimum 1000 votes\n",
    "df_filtered = df_joined.filter(\n",
    "    (col(\"titleType\") == \"movie\") &\n",
    "    (col(\"genres\").contains(\"Comedy\")) &\n",
    "    (col(\"numVotes\") >= 1000)\n",
    ")\n",
    "\n",
    "w = Window.orderBy(desc(\"averageRating\"), desc(\"numVotes\"))\n",
    "\n",
    "highest_rated_comedy_movie = (\n",
    "    df_filtered\n",
    "    .withColumn(\"rank\", dense_rank().over(w))\n",
    "    .filter(col(\"rank\") == 1)\n",
    "    .drop(\"rank\")\n",
    ")\n",
    "\n",
    "highest_rated_comedy_movie.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1757e235-ef10-4dc3-b550-9cd7714b7e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**13.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b76e1a7-3820-40f0-b73e-69cdebf75076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 20:22:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:22:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:22:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:22:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:22:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:22:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:22:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/19 20:22:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|primaryName|\n",
      "+-----------+\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, explode\n",
    "\n",
    "# Joindre avec title.crew\n",
    "highest_rated_comedy_movie_with_crew = (\n",
    "    highest_rated_comedy_movie\n",
    "    .join(dict_df[\"title.crew\"], on=\"tconst\", how=\"inner\")\n",
    ")\n",
    "\n",
    "# Exploser les directors (car ils peuvent être multiples, séparés par des virgules)\n",
    "directors_exploded = (\n",
    "    highest_rated_comedy_movie_with_crew\n",
    "    .withColumn(\"director_id\", explode(split(col(\"directors\"), \",\")))\n",
    ")\n",
    "\n",
    "highest_rated_comedy_movie_director_df = (\n",
    "    directors_exploded\n",
    "    .join(dict_df[\"name.basics\"], directors_exploded.director_id == dict_df[\"name.basics\"].nconst, how=\"inner\")\n",
    "    .select(\"primaryName\")\n",
    ")\n",
    "\n",
    "highest_rated_comedy_movie_director_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32bd84c7-91f9-47a1-816a-516ce7cebd35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**14**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c4ca3a5-64cb-489d-99c0-f95992acfb37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For this last question we suspect that it was based on the fields provided in the _title.principals_ dataset (that we couldn't download at the beginning of this notebook since it causes OOM in Databricks...)\n",
    "\n",
    "But we could easily imagine that the query to obtain the alternate titles would be something like:  \n",
    "<br>\n",
    "```\n",
    "highest_rated_comedy_movie_titles_df = (\n",
    "    highest_rated_comedy_movie\n",
    "    .join(dict_df[\"title.principals\"], on=\"tconst\", how=\"inner\")\n",
    "    .select(\"primaryTitle\", \"alternatesTitle\")\n",
    ")\n",
    "\n",
    "display(highest_rated_comedy_movie_titles_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09194b8-bbdb-4b2b-ac2c-6d550db37cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05e7ff7-bc5a-4ed3-9237-e24390c1c220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Stream Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This section implements a real-time stream processing job that monitors Wikipedia edits for five entities from our IMDB dataset using the Wikimedia EventStreams API via `pywikibot`.\n",
    "\n",
    "**Selected Entities (from IMDB dataset):**\n",
    "1. **Christopher Nolan** - Director\n",
    "2. **The Shawshank Redemption** - Top-rated movie\n",
    "3. **Leonardo DiCaprio** - Actor\n",
    "4. **Star Wars** - Movie franchise\n",
    "5. **Science fiction** - Genre\n",
    "\n",
    "**Metrics Tracked:**\n",
    "- Edit count per entity\n",
    "- Unique editors count\n",
    "- Last edit timestamp\n",
    "\n",
    "**Alert System:**\n",
    "Every 5 edits on a tracked entity triggers an alert saved to `alerts.json`.\n",
    "\n",
    "**Output Files:**\n",
    "- `stream_metrics.json` - Metrics for all tracked entities\n",
    "- `alerts.json` - High-frequency edit alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install pywikibot requests-sse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywikibot in ./venv/lib/python3.13/site-packages (10.7.4)\n",
      "Requirement already satisfied: requests-sse in ./venv/lib/python3.13/site-packages (0.5.2)\n",
      "Requirement already satisfied: mwparserfromhell>=0.5.2 in ./venv/lib/python3.13/site-packages (from pywikibot) (0.7.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.13/site-packages (from pywikibot) (25.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./venv/lib/python3.13/site-packages (from pywikibot) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests>=2.31.0->pywikibot) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests>=2.31.0->pywikibot) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests>=2.31.0->pywikibot) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests>=2.31.0->pywikibot) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pywikibot requests-sse\n",
    "# Restart the kernel manually: Kernel > Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking entities:\n",
      "  - Christopher Nolan (director)\n",
      "  - The Shawshank Redemption (movie)\n",
      "  - Leonardo DiCaprio (actor)\n",
      "  - Star Wars (franchise)\n",
      "  - Science fiction (genre)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pywikibot.comms.eventstreams import EventStreams\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "# 5 entities from IMDB dataset to track\n",
    "TRACKED_ENTITIES = {\n",
    "    \"Christopher Nolan\": \"director\",\n",
    "    \"The Shawshank Redemption\": \"movie\",\n",
    "    \"Leonardo DiCaprio\": \"actor\",\n",
    "    \"Star Wars\": \"franchise\",\n",
    "    \"Science fiction\": \"genre\"\n",
    "}\n",
    "\n",
    "# Initialize storage\n",
    "metrics = defaultdict(lambda: {\"edit_count\": 0, \"unique_editors\": set(), \"last_edit\": None})\n",
    "alerts = []\n",
    "\n",
    "print(\"Tracking entities:\")\n",
    "for entity, etype in TRACKED_ENTITIES.items():\n",
    "    print(f\"  - {entity} ({etype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output paths\n",
    "METRICS_FILE = \"stream_metrics.json\"\n",
    "ALERTS_FILE = \"alerts.json\"\n",
    "LAST_EVENT_CACHE = \"last_event_cache.txt\"\n",
    "\n",
    "def check_file_exists(path: str) -> bool:\n",
    "    return os.path.exists(path)\n",
    "\n",
    "def set_stream(start_time: datetime) -> EventStreams:\n",
    "    \"\"\"Initialize stream from cache or from 1 day ago\"\"\"\n",
    "    if check_file_exists(LAST_EVENT_CACHE):\n",
    "        with open(LAST_EVENT_CACHE, 'r') as f:\n",
    "            return EventStreams(streams=[\"recentchange\"], since=f.read().strip())\n",
    "    else:\n",
    "        since_date = (start_time - timedelta(days=1)).strftime('%Y%m%d')\n",
    "        return EventStreams(streams=[\"recentchange\"], since=since_date)\n",
    "\n",
    "def save_metrics():\n",
    "    \"\"\"Save metrics and alerts to JSON files\"\"\"\n",
    "    output = {entity: {\"type\": TRACKED_ENTITIES[entity], \"edit_count\": data[\"edit_count\"],\n",
    "                       \"unique_editors\": len(data[\"unique_editors\"]), \"last_edit\": data[\"last_edit\"]}\n",
    "              for entity, data in metrics.items()}\n",
    "    \n",
    "    with open(METRICS_FILE, \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    with open(ALERTS_FILE, \"w\") as f:\n",
    "        json.dump(alerts, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved: {METRICS_FILE}, {ALERTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event(event: dict) -> bool:\n",
    "    \"\"\"Process event and check if it matches tracked entities\"\"\"\n",
    "    title = event.get(\"title\", \"\")\n",
    "    user = event.get(\"user\", \"anonymous\")\n",
    "    timestamp = event.get(\"meta\", {}).get(\"dt\", datetime.now().isoformat())\n",
    "    \n",
    "    for entity in TRACKED_ENTITIES:\n",
    "        if entity.lower() in title.lower():\n",
    "            metrics[entity][\"edit_count\"] += 1\n",
    "            metrics[entity][\"unique_editors\"].add(user)\n",
    "            metrics[entity][\"last_edit\"] = timestamp\n",
    "            \n",
    "            print(f\"Match: {entity} - '{title}' by {user}\")\n",
    "            \n",
    "            # Alert every 5 edits\n",
    "            if metrics[entity][\"edit_count\"] % 5 == 0:\n",
    "                alerts.append({\n",
    "                    \"entity\": entity,\n",
    "                    \"type\": TRACKED_ENTITIES[entity],\n",
    "                    \"edit_count\": metrics[entity][\"edit_count\"],\n",
    "                    \"timestamp\": timestamp\n",
    "                })\n",
    "                print(f\"ALERT: {entity} reached {metrics[entity]['edit_count']} edits!\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stream processing for 5 minutes...\n",
      "Stop time: 20:23:40\n",
      "\n",
      "Progress: 100 events (0 matched)\n",
      "Progress: 200 events (0 matched)\n",
      "Progress: 300 events (0 matched)\n",
      "Progress: 400 events (0 matched)\n",
      "Progress: 500 events (0 matched)\n",
      "Progress: 600 events (0 matched)\n",
      "Progress: 700 events (0 matched)\n",
      "Progress: 800 events (0 matched)\n",
      "Progress: 900 events (0 matched)\n",
      "Progress: 1000 events (0 matched)\n",
      "Progress: 1100 events (0 matched)\n",
      "Progress: 1200 events (0 matched)\n",
      "Progress: 1300 events (0 matched)\n",
      "Progress: 1400 events (0 matched)\n",
      "Progress: 1500 events (0 matched)\n",
      "Progress: 1600 events (0 matched)\n",
      "Progress: 1700 events (0 matched)\n",
      "Progress: 1800 events (0 matched)\n",
      "Progress: 1900 events (0 matched)\n",
      "Progress: 2000 events (0 matched)\n",
      "Progress: 2100 events (0 matched)\n",
      "Progress: 2200 events (0 matched)\n",
      "Progress: 2300 events (0 matched)\n",
      "Progress: 2400 events (0 matched)\n",
      "Progress: 2500 events (0 matched)\n",
      "Progress: 2600 events (0 matched)\n",
      "Progress: 2700 events (0 matched)\n",
      "Progress: 2800 events (0 matched)\n",
      "Progress: 2900 events (0 matched)\n",
      "Progress: 3000 events (0 matched)\n",
      "Progress: 3100 events (0 matched)\n",
      "Progress: 3200 events (0 matched)\n",
      "Progress: 3300 events (0 matched)\n",
      "Progress: 3400 events (0 matched)\n",
      "Progress: 3500 events (0 matched)\n",
      "Progress: 3600 events (0 matched)\n",
      "Progress: 3700 events (0 matched)\n",
      "Progress: 3800 events (0 matched)\n",
      "Progress: 3900 events (0 matched)\n",
      "Progress: 4000 events (0 matched)\n",
      "Progress: 4100 events (0 matched)\n",
      "Progress: 4200 events (0 matched)\n",
      "Progress: 4300 events (0 matched)\n",
      "Progress: 4400 events (0 matched)\n",
      "Progress: 4500 events (0 matched)\n",
      "Progress: 4600 events (0 matched)\n",
      "Progress: 4700 events (0 matched)\n",
      "Progress: 4800 events (0 matched)\n",
      "Progress: 4900 events (0 matched)\n",
      "Progress: 5000 events (0 matched)\n",
      "Progress: 5100 events (0 matched)\n",
      "Progress: 5200 events (0 matched)\n",
      "Progress: 5300 events (0 matched)\n",
      "Progress: 5400 events (0 matched)\n",
      "Progress: 5500 events (0 matched)\n",
      "Progress: 5600 events (0 matched)\n",
      "Progress: 5700 events (0 matched)\n",
      "Progress: 5800 events (0 matched)\n",
      "Progress: 5900 events (0 matched)\n",
      "Progress: 6000 events (0 matched)\n",
      "Progress: 6100 events (0 matched)\n",
      "Progress: 6200 events (0 matched)\n",
      "Progress: 6300 events (0 matched)\n",
      "Progress: 6400 events (0 matched)\n",
      "Progress: 6500 events (0 matched)\n",
      "Progress: 6600 events (0 matched)\n",
      "Progress: 6700 events (0 matched)\n",
      "Progress: 6800 events (0 matched)\n",
      "Progress: 6900 events (0 matched)\n",
      "Progress: 7000 events (0 matched)\n",
      "Progress: 7100 events (0 matched)\n",
      "Progress: 7200 events (0 matched)\n",
      "Progress: 7300 events (0 matched)\n",
      "Progress: 7400 events (0 matched)\n",
      "Progress: 7500 events (0 matched)\n",
      "Progress: 7600 events (0 matched)\n",
      "Progress: 7700 events (0 matched)\n",
      "Progress: 7800 events (0 matched)\n",
      "Progress: 7900 events (0 matched)\n",
      "Progress: 8000 events (0 matched)\n",
      "Progress: 8100 events (0 matched)\n",
      "Progress: 8200 events (0 matched)\n",
      "Progress: 8300 events (0 matched)\n",
      "Progress: 8400 events (0 matched)\n",
      "Progress: 8500 events (0 matched)\n",
      "Progress: 8600 events (0 matched)\n",
      "Progress: 8700 events (0 matched)\n",
      "Progress: 8800 events (0 matched)\n",
      "Progress: 8900 events (0 matched)\n",
      "Progress: 9000 events (0 matched)\n",
      "Progress: 9100 events (0 matched)\n",
      "Progress: 9200 events (0 matched)\n",
      "Progress: 9300 events (0 matched)\n",
      "Progress: 9400 events (0 matched)\n",
      "Progress: 9500 events (0 matched)\n",
      "Progress: 9600 events (0 matched)\n",
      "Progress: 9700 events (0 matched)\n",
      "Progress: 9800 events (0 matched)\n",
      "Progress: 9900 events (0 matched)\n",
      "Progress: 10000 events (0 matched)\n",
      "Progress: 10100 events (0 matched)\n",
      "Progress: 10200 events (0 matched)\n",
      "Progress: 10300 events (0 matched)\n",
      "Progress: 10400 events (0 matched)\n",
      "Progress: 10500 events (0 matched)\n",
      "Progress: 10600 events (0 matched)\n",
      "Progress: 10700 events (0 matched)\n",
      "Progress: 10800 events (0 matched)\n",
      "Progress: 10900 events (0 matched)\n",
      "Progress: 11000 events (0 matched)\n",
      "Progress: 11100 events (0 matched)\n",
      "Progress: 11200 events (0 matched)\n",
      "Progress: 11300 events (0 matched)\n",
      "Progress: 11400 events (0 matched)\n",
      "Progress: 11500 events (0 matched)\n",
      "Progress: 11600 events (0 matched)\n",
      "Progress: 11700 events (0 matched)\n",
      "Progress: 11800 events (0 matched)\n",
      "Progress: 11900 events (0 matched)\n",
      "Progress: 12000 events (0 matched)\n",
      "Progress: 12100 events (0 matched)\n",
      "Match: Star Wars - 'List of Star Wars: Young Jedi Adventures episodes' by Andykatib\n",
      "Progress: 12200 events (1 matched)\n",
      "Progress: 12300 events (1 matched)\n",
      "Match: Star Wars - 'Star Wars Outlaws' by OceanHok\n",
      "Progress: 12400 events (2 matched)\n",
      "Match: Star Wars - 'Star Wars Outlaws' by OceanHok\n",
      "Progress: 12500 events (3 matched)\n",
      "Progress: 12600 events (3 matched)\n",
      "Progress: 12700 events (3 matched)\n",
      "Progress: 12800 events (3 matched)\n",
      "Progress: 12900 events (3 matched)\n",
      "Progress: 13000 events (3 matched)\n",
      "Progress: 13100 events (3 matched)\n",
      "Progress: 13200 events (3 matched)\n",
      "Progress: 13300 events (3 matched)\n",
      "Progress: 13400 events (3 matched)\n",
      "Progress: 13500 events (3 matched)\n",
      "Progress: 13600 events (3 matched)\n",
      "Match: Star Wars - 'Star Wars Trilogy Arcade' by DreamRimmer bot II\n",
      "Progress: 13700 events (4 matched)\n",
      "Progress: 13800 events (4 matched)\n",
      "Progress: 13900 events (4 matched)\n",
      "Progress: 14000 events (4 matched)\n",
      "Progress: 14100 events (4 matched)\n",
      "Progress: 14200 events (4 matched)\n",
      "Progress: 14300 events (4 matched)\n",
      "Progress: 14400 events (4 matched)\n",
      "Progress: 14500 events (4 matched)\n",
      "Progress: 14600 events (4 matched)\n",
      "Progress: 14700 events (4 matched)\n",
      "Progress: 14800 events (4 matched)\n",
      "Progress: 14900 events (4 matched)\n",
      "Progress: 15000 events (4 matched)\n",
      "Progress: 15100 events (4 matched)\n",
      "Progress: 15200 events (4 matched)\n",
      "Progress: 15300 events (4 matched)\n",
      "Progress: 15400 events (4 matched)\n",
      "Progress: 15500 events (4 matched)\n",
      "Progress: 15600 events (4 matched)\n",
      "Progress: 15700 events (4 matched)\n",
      "Progress: 15800 events (4 matched)\n",
      "Progress: 15900 events (4 matched)\n",
      "Progress: 16000 events (4 matched)\n",
      "Progress: 16100 events (4 matched)\n",
      "Progress: 16200 events (4 matched)\n",
      "Progress: 16300 events (4 matched)\n",
      "Progress: 16400 events (4 matched)\n",
      "Match: Star Wars - 'Star Wars: Episode I – The Phantom Menace' by HerbertGP36\n",
      "ALERT: Star Wars reached 5 edits!\n",
      "Progress: 16500 events (5 matched)\n",
      "Progress: 16600 events (5 matched)\n",
      "Progress: 16700 events (5 matched)\n",
      "Progress: 16800 events (5 matched)\n",
      "Progress: 16900 events (5 matched)\n",
      "Progress: 17000 events (5 matched)\n",
      "Progress: 17100 events (5 matched)\n",
      "Progress: 17200 events (5 matched)\n",
      "Progress: 17300 events (5 matched)\n",
      "Progress: 17400 events (5 matched)\n",
      "Progress: 17500 events (5 matched)\n",
      "Progress: 17600 events (5 matched)\n",
      "Progress: 17700 events (5 matched)\n",
      "Progress: 17800 events (5 matched)\n",
      "Progress: 17900 events (5 matched)\n",
      "Progress: 18000 events (5 matched)\n",
      "Progress: 18100 events (5 matched)\n",
      "Progress: 18200 events (5 matched)\n",
      "Progress: 18300 events (5 matched)\n",
      "Progress: 18400 events (5 matched)\n",
      "Progress: 18500 events (5 matched)\n",
      "Progress: 18600 events (5 matched)\n",
      "Progress: 18700 events (5 matched)\n",
      "Progress: 18800 events (5 matched)\n",
      "Progress: 18900 events (5 matched)\n",
      "Progress: 19000 events (5 matched)\n",
      "Progress: 19100 events (5 matched)\n",
      "Progress: 19200 events (5 matched)\n",
      "Progress: 19300 events (5 matched)\n",
      "Progress: 19400 events (5 matched)\n",
      "Progress: 19500 events (5 matched)\n",
      "Progress: 19600 events (5 matched)\n",
      "Progress: 19700 events (5 matched)\n",
      "Progress: 19800 events (5 matched)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m datetime.now() < stop_time:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m         event = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m         event_count += \u001b[32m1\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m process_event(event):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:360\u001b[39m, in \u001b[36m__next__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/pywikibot/tools/collections.py:285\u001b[39m, in \u001b[36mGeneratorWrapper.send\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m_started_gen\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    283\u001b[39m     \u001b[38;5;66;03m# start the generator\u001b[39;00m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28mself\u001b[39m._started_gen = \u001b[38;5;28mself\u001b[39m.generator\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_started_gen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/pywikibot/comms/eventstreams.py:387\u001b[39m, in \u001b[36mEventStreams.generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28mself\u001b[39m.source.connect(config.max_retries)\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     event = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m, httplib.IncompleteRead) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    389\u001b[39m     warning(\n\u001b[32m    390\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mConnection error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTry to re-establish connection.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/requests_sse/client.py:203\u001b[39m, in \u001b[36mEventSource.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         line_in_bytes = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    205\u001b[39m         \u001b[38;5;28mself\u001b[39m._event_type = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/urllib3/response.py:1246\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1231\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1243\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1244\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[32m   1249\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp)\n\u001b[32m   1250\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m\n\u001b[32m   1251\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.has_unconsumed_tail)\n\u001b[32m   1252\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/urllib3/response.py:1414\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1412\u001b[39m     chunk = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1414\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1416\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/big-data/big-data-processing-project/venv/lib/python3.13/site-packages/urllib3/response.py:1329\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1330\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.9_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Stream configuration\n",
    "start_time = datetime.now()\n",
    "duration_minutes = 5\n",
    "stop_time = start_time + timedelta(minutes=duration_minutes)\n",
    "\n",
    "print(f\"Starting stream processing for {duration_minutes} minutes...\")\n",
    "print(f\"Stop time: {stop_time.strftime('%H:%M:%S')}\\n\")\n",
    "\n",
    "# Initialize stream (filter for English Wikipedia edits)\n",
    "stream = set_stream(start_time)\n",
    "stream.register_filter(server_name='en.wikipedia.org', type='edit')\n",
    "\n",
    "# Process events\n",
    "event_count = matched_count = 0\n",
    "\n",
    "while datetime.now() < stop_time:\n",
    "    try:\n",
    "        event = next(stream)\n",
    "        event_count += 1\n",
    "        \n",
    "        if process_event(event):\n",
    "            matched_count += 1\n",
    "        \n",
    "        # Update cache\n",
    "        event_timestamp = event.get('meta', {}).get('dt', '')\n",
    "        if event_timestamp:\n",
    "            with open(LAST_EVENT_CACHE, 'w') as f:\n",
    "                f.write(event_timestamp)\n",
    "        \n",
    "        # Progress every 100 events\n",
    "        if event_count % 100 == 0:\n",
    "            print(f\"Progress: {event_count} events ({matched_count} matched)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save results\n",
    "save_metrics()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n--- SUMMARY ---\")\n",
    "print(f\"Duration: {duration_minutes} min | Events: {event_count} | Matched: {matched_count} | Alerts: {len(alerts)}\")\n",
    "print(\"\\nResults:\")\n",
    "for entity, data in sorted(metrics.items(), key=lambda x: x[1][\"edit_count\"], reverse=True):\n",
    "    print(f\"  {entity}: {data['edit_count']} edits, {len(data['unique_editors'])} editors\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
